{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RiWOqVGJpnR5",
    "outputId": "fdde8e36-e354-4943-d027-e8a691e184ec"
   },
   "source": [
    "# Introduction to Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pobAbXhxpnRm"
   },
   "source": [
    "## Introduction:  \n",
    "\n",
    "All machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Hence a need for feature engineering arises. Feature engineering efforts mainly have two goals:\n",
    "- Preparing the proper input dataset, compatible with the machine learning algorithm requirements.\n",
    "- Improving the performance of machine learning models.\n",
    "\n",
    "In this webinar we will summarizes the main techniques of feature engineering that are most commonly used. Some techniques might work better with some algorithms or datasets, while some of them might be beneficial in all cases. The best way to achieve expertise in feature engineering is practicing different techniques on various datasets and observing their effect on model performances.\n",
    "\n",
    "**List of Techniques**\n",
    "1. Imputation\n",
    "2. Handling Outliers\n",
    "3. Binning\n",
    "4. Log Transform\n",
    "5. One-Hot Encoding\n",
    "6. Grouping Operations\n",
    "7. Feature Split\n",
    "8. Scaling\n",
    "\n",
    "All we use is NumPy and Pandas to do perform Feature Engineering to sort out the useful data from the mess. \n",
    "\n",
    "<img src=\"https://img-a.udemycdn.com/course/750x422/1304050_ee0f_8.jpg\" alt=\"Numpy Logo\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *1. Imputation*\n",
    "\n",
    "Missing values are one of the most common problems you can encounter when you try to prepare your data for machine learning. The reason for the missing values might be human errors, interruptions in the data flow, privacy concerns, and so on. Whatever is the reason, missing values affect the performance of the machine learning models.\n",
    "\n",
    "The most simple solution to the missing values is to drop the rows or the entire column. There is not an optimum threshold for dropping but you can use 70% as an example value and try to drop the rows and columns which have missing values with higher than this threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "threshold = 0.7\n",
    "\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Imputation\n",
    "\n",
    "Imputation is a more preferable option rather than dropping because it preserves the data size. However, there is an important selection of what you impute to the missing values.\n",
    "The best imputation way is to use the medians of the columns. As the averages of the columns are sensitive to the outlier values, while medians are more solid in this respect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Imputation\n",
    "\n",
    "Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *2. Handling Outliers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection with Standard Deviation\n",
    "\n",
    "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\n",
    "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical. In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Detection with Percentiles\n",
    "\n",
    "Another mathematical method to detect outliers is to use percentiles. You can assume a certain percent of the value from the top or the bottom as an outlier. The key point is here to set the percentage value once again, and this depends on the distribution of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Outlier Dilemma: Drop or Cap\n",
    "\n",
    "Another option for handling outliers is to cap them instead of dropping. So you can keep your data size and at the end of the day, it might be better for the final model performance.\n",
    "On the other hand, capping can affect the distribution of the data, thus it better not to exaggerate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *3. Data Binning*\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://miro.medium.com/max/702/0*XWta_U67Nv9udfY-.png\"></div>\n",
    "\n",
    "Data binning (also called Discrete binning or bucketing) is a data pre-processing technique used to reduce the effects of minor observation errors. The original data values which fall into a given small interval, a bin, are replaced by a value representative of that interval, often the central value. It is a form of quantization. The main motivation of data binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. The trade-off between performance and overfitting is the key point of the binning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Numerical Binning Example\n",
    "\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT\n",
    "```\n",
    "    value   bin\n",
    "0      2   Low\n",
    "1     45   Mid\n",
    "2      7   Low\n",
    "3     85  High\n",
    "4     28   Low\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INPUT:\n",
    "```\n",
    "#Categorical Binning Example\n",
    "     Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT:\n",
    "```\n",
    "     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *4. Log Transform*\n",
    "\n",
    "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. Benefits log transform:\n",
    "- It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
    "- In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.\n",
    "- It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *5. One-hot encoding*\n",
    "\n",
    "One-hot encoding is one of the most common encoding methods in machine learning. This method spreads the values in a column to multiple flag columns and assigns 0 or 1 to them. These binary values express the relationship between grouped and encoded column.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://miro.medium.com/max/702/1*ZX99GOZ6-9_yJg6rZchTEA.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *6. Grouping Operations*\n",
    "\n",
    "In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called “Tidy”. Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Column Grouping\n",
    "\n",
    "There are three different ways for aggregating categorical columns:\n",
    "\n",
    "- The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference.\n",
    "\n",
    "<div style=\"text-align:center\"><img src=\"https://miro.medium.com/max/702/1*VWBbZRkTrHJQrQfWlPQWUg.png\"></div>\n",
    "\n",
    "```python\n",
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Column Grouping\n",
    "\n",
    "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature.\n",
    "\n",
    "```python\n",
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *7. Feature Split*\n",
    "\n",
    "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\n",
    "- We enable machine learning algorithms to comprehend them.\n",
    "- Make possible to bin and group them.\n",
    "- Improve model performance by uncovering potential information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *8. Scaling*\n",
    "\n",
    "There are two common ways of scaling:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.\n",
    "\n",
    "```python\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.\n",
    "\n",
    "```python\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "We are going to see how to quickly and efficiently use the algorithms that we have studied till now. for that we are going to use [Scikit-learn](https://scikit-learn.org/stable/) which a python library in which we are going to see how to use:\n",
    " \n",
    "1. Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Support Vector Machines (SVM)\n",
    "4. K-means Clustering\n",
    "5. Naive Bayes\n",
    " \n",
    "We are not going to show Decision trees as we already showed the scikit-learn implementation of that in the DecisionTrees and NaiveBayes Notebook.\n",
    " \n",
    "The library has a host of other algorithms implemented that we won't be covering to keep things inline but do checkout scikit learn's documentation for that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, datasets\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "diabetes_X, diabetes_y = datasets.load_diabetes(return_X_y=True)\n",
    "\n",
    "# Use only one feature\n",
    "diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "diabetes_X_train = diabetes_X[:-20]\n",
    "diabetes_X_test = diabetes_X[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets\n",
    "diabetes_y_train = diabetes_y[:-20]\n",
    "diabetes_y_test = diabetes_y[-20:]\n",
    "\n",
    "regression = linear_model.LinearRegression()\n",
    "\n",
    "# Train the regression model\n",
    "regression.fit(diabetes_X_train, diabetes_y_train)\n",
    "\n",
    "# Make predictions\n",
    "diabetes_y_pred = regression.predict(diabetes_X_test)\n",
    "\n",
    "# Plot the linear line fitted by the model\n",
    "plt.scatter(diabetes_X_test, diabetes_y_test,  color='orange')\n",
    "plt.plot(diabetes_X_test, diabetes_y_pred, color='green', linewidth=3)\n",
    "\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Loading the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=0, max_iter=130)\n",
    "\n",
    "# Training the model\n",
    "log_reg.fit(X, y)\n",
    "\n",
    "print(\"Accuracy: \"+ str(log_reg.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines(Classification)\n",
    " \n",
    "In this example we are going to use SVM from scikit-learn and also do some preprocessing which is just subtracting the data by the mean and dividing by standard deviation and create a pipeline so you don't need to pass the data to the the preprocessing step and then to SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9733333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Creating the pipeline with first the prepocessing\n",
    "# and then SVM\n",
    "svm_pipeline = make_pipeline(StandardScaler(),\n",
    "                    SVC(gamma='auto'))\n",
    "\n",
    "# Training the model\n",
    "svm_pipeline.fit(X, y)\n",
    "\n",
    "print(\"Accuracy: \"+ str(svm_pipeline.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see both Support Vector Machines and logistic regression were able to do good on a simple dataset. but SVMs generally outperform logistic regression in some conditions because the type of problem the SVM tries to solve is always convex and therefore always reaches a global minima and doesn't get stuck in local minima. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "# Initializing fake data\n",
    "X = np.array([[1, 2], [1, 4], [1, 0],\n",
    "              [10, 2], [10, 4], [10, 0]])\n",
    "\n",
    "# n_clusters is the number of clusters you want to \n",
    "# divide the data in\n",
    "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
    "\n",
    "# Shows which data point was categorized into which cluster\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 75 points : 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Number of mislabeled points out of a total %d points : %d\"\n",
    "      % (X_test.shape[0], (y_test != y_pred).sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anything and everything you need to know to use Scikit Learn](https://scikit-learn.org/stable/user_guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are Neural Networks?\n",
    "\n",
    "[Neural Networks, forward and backward propagation](https://www.investopedia.com/terms/n/neuralnetwork.asp#:~:text=Neural%20networks%20are%20a%20series,fraud%20detection%20and%20risk%20assessment.) \n",
    "\n",
    "[Gradient Descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)\n",
    "\n",
    "[Activation Functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function\n",
    " \n",
    "Where **$ g $** is a sigmoid function in our example and these type of functions are called **Activation functions** in a neural network, the purpose of a activation function in a neural network is to induce non- linearity, let's see what I mean by that when we calculate $ Z $ or $Y\\_hat $ we always calculate a linear function of the form $$ y = mx + b $$ which if you remember is an equation of a line and that doesn't seems much non-linear to me and to fit the data better we include such activation functions which help in fitting complex functions to our data so we have better prediction or better accuracy at the end. \n",
    " \n",
    "<img src=\"./assets/sigmoid.png\" width=300/>\n",
    " \n",
    "A sigmoid function in python can be implemented something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of -2 is 0.11920292202211755\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Sigmoid')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def Sigmoid(x):\n",
    "    return (1 / (1 + np.exp(-x)))\n",
    "            \n",
    "# change the value here to see different output.\n",
    "inp = -2\n",
    "print(\"Sigmoid of \" + str(inp) + \" is \" + str(Sigmoid(inp)))\n",
    "\n",
    "# Plot of Sigmoid function\n",
    "inp = np.linspace(-5, 5)\n",
    "plt.plot(inp, Sigmoid(inp))\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Sigmoid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function \n",
    " \n",
    "So now as we know how to go through a step of forward Propagation let's take an example from our dummy dataset and try to understand how to find the hypothesis function $ h(X) $.\n",
    " \n",
    "For this example we will assume that the values of $ W_1, b_1, W_2 $ and $ b_2 $ are $ 0.009, -0.0140, 2700 $ and $ 437 $ respectively, The values of of the parameters $ W $ and $ b $ in an actual neural network are not so extreme such as some in decimal and some in thousands. \n",
    " \n",
    "Substituting all the values in a single step of forward propagation we get.\n",
    " \n",
    "1. $ Z = 0.0009 * 905 + -0.0140 = 0.8005 $\n",
    "2. $ A = g(0.8005) = 0.6900 $\n",
    "3. $ Y\\_hat = 2700 * 0.6900 + 456 = 2319 $\n",
    " \n",
    "$ Y\\_hat $ which is equal to 2319 seems to be pretty close to the actual value of 2500 but how do we adjust the parameters to get more close to the actual value how should we measure the difference or the **loss**.\n",
    " \n",
    "So the way we calculate the loss through a loss function called **Mean Square Error** is like this.\n",
    " \n",
    "<img src=\"./assets/mse.png\" width=300/>\n",
    " \n",
    "> From [Wikipeadia](https://en.wikipedia.org/wiki/Mean_squared_error#:~:text=In%20statistics%2C%20the%20mean%20squared,values%20and%20the%20actual%20value.)\n",
    " \n",
    "The loss for our example will be:\n",
    " \n",
    "$$ 1/2 *  Σ ((2500 - 2319)^2 ) = 32761 $$\n",
    " \n",
    "As we just had a single example $ n $ is equal to $ 1 $ and in deep learning we generally divide MSE with $ 2 $. A simple Implementation of the Mean Squared Error can be done something like this. This $ Σ $ charater is called and is used to denote sum of an vector in our case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16380.5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def MSE(Y, Y_hat):\n",
    "    n = len(Y)\n",
    "    return (np.sum((Y - Y_hat)**2)) / (2 * n)\n",
    "\n",
    "# Try an array of numbers or just change the inputs to MSE to see how the output changes.\n",
    "Y = np.array([2500])\n",
    "Y_hat = np.array([2319])\n",
    "\n",
    "MSE(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we do\n",
    " \n",
    "In this notebook we are going to implement a neural network from scratch in python and this time we are going to build a different neural network then the one that we used to explain neural networks. This time we are going to solve a classification problem. So there will be some minor changes to the network to use it for Classification. Let's look at them.\n",
    " \n",
    "## Softmax \n",
    " \n",
    "A softmax function takes a vector of input and gives out a vector with probabilities that obviously add up to one because they are probabilities and in our case a softmax unit at the end will tell us the probability of the class that the neural network thinks it is.\n",
    " \n",
    "Softmax is calculated by taking the elementwise exponent of the vector and then dividing elementwise by the sum of the vector after taking the exponent.\n",
    " \n",
    "<img src=\"./assets/softmax.png\" width=150>\n",
    " \n",
    " \n",
    "so let's take an example in our implementation we will be using the iris dataset which has 3 flowers['setosa', 'versicolor', 'virginica'] in the same order so for one example the true output looks like this.\n",
    " \n",
    "$$ Y = [0, 1, 0] $$\n",
    " \n",
    "This means that the second node in our neural network denotes a particular flower which is versicolor and we will be expecting a softmax output something like this.\n",
    " \n",
    "$$ Y_hat = [0.3, 0.95, 0.2] $$\n",
    " \n",
    "We pick the largest number in the vector to decide which class the neural network thinks this example denotes to.\n",
    " \n",
    "## Loss Function\n",
    " \n",
    "We will also use a new loss function named **Cross Entropy loss** to measure the classification loss between multiple classes.\n",
    " \n",
    "the formula for the loss is this where $ y_o $ is Y or the labels and $ P_o $ is the predicted label which you will also see in the below implementation. \n",
    " \n",
    "<img src=\"./assets/cross_entropy.png\" width=150>\n",
    " \n",
    " \n",
    "we will change the input units or the inputs($ X $) to 4 and the number of hidden units to 5 and the output is going to be 3 units as there are 3 flowers Now let's see the architecture of the neural network.\n",
    " \n",
    "<img src=\"./assets/nn.png\" width=700>\n",
    " \n",
    "The Forward, Backward and Gradient steps are going to remain the same, the only difference is that now instead of a scalar they are going to be matrices. Let's have a look over these steps again and I will be mentioning the matrix dimensions in front of all the variables this time.\n",
    " \n",
    "### Forward Propagation step\n",
    " \n",
    "As we know that we will be using a sigmoid function instead of $ g() $ and a softmax function at the end I will also be replacing that.\n",
    " \n",
    "1. $ Z_1 = W_1[5,4] * X[4,150] + b_1[5, 1]$\n",
    "2. $ A_1 = sigmoid(Z)[5, 150] $\n",
    "3. $ Z_2 = softmax(W_2[3, 5] * A_1[5,150] + b_2[3 ,1])$\n",
    "4. $ Y\\_hat = Z_2[3, 150] $\n",
    " \n",
    "### Backward Propagation step\n",
    " \n",
    "For backward Propagation we only used $ W1 $ as our example and the change their is that instead of calculating the derivative for a scalar we will be calculating the derivative for the whole vector or matrix but element wise. Let's take an example and suppose we had a vector A like the one below. \n",
    " \n",
    "$$ A = [2, 4, 5] $$\n",
    " \n",
    "So if say do an element wise square on $ A $, The result will be.\n",
    " \n",
    "$$ A^2 = [4, 16, 25] $$\n",
    " \n",
    "And as the loss function is changed the derivative of $ ∂E/∂Y\\_hat $ is also changed.\n",
    " \n",
    "1. $ ∂E/∂Y\\_hat = Y_hat - Y $\n",
    "2. $ ∂Y\\_hat/∂A = W_2 $\n",
    "3. $ ∂A/∂Z = A*(1 - A) $\n",
    "4. $ ∂Z/∂W_1 = X $\n",
    " \n",
    "finally the whole derivative for $ ∂E/∂W_1 $ will be.\n",
    " \n",
    "$$ ∂E/∂W_1 = (Y_hat - Y) * W_2 * A*(1 - A) * X $$\n",
    " \n",
    "### Gradient descent\n",
    " \n",
    "$$ W_1[5, 4] = W_1[5, 4] - α[1, 1] * ∂E/∂W_1[5, 4] $$ \n",
    "$$ b_1[5, 1] = b_1[5, 1] - α[1, 1] * ∂E/∂b_1[5, 1] $$\n",
    "$$ W_2[3, 5] = W_2[3, 5] - α[1, 1] * ∂E/∂W_2[3, 5] $$\n",
    "$$ b_2[3, 1] = b_2[3, 1] - α[1, 1] * ∂E/∂b_2[3, 1] $$\n",
    " \n",
    "Where $ α $ is going to be a scalar which we will broadcast to match the shape of our matrix to which it subtracts with. \n",
    " \n",
    "## Implementation\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "def sigmoid(X):\n",
    "  return 1 / (1 + np.exp(-X))\n",
    "\n",
    "def sigmoid_der(x):\n",
    "  return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def softmax(X):\n",
    "  e = np.exp(X - X.max())\n",
    "  return (e / e.sum(axis=1, keepdims=True))\n",
    "\n",
    "def loss(Y, Y_hat):\n",
    "  loss = Y * np.log(Y_hat)\n",
    "  return -np.sum(loss)\n",
    "\n",
    "def neural_network_train(X, Y, num_iteration=1200):\n",
    "  # Random initializing the weights and bias\n",
    "\n",
    "  W_1 = np.random.randn(5, 4)\n",
    "  b_1 = np.random.randn(1, 5)\n",
    "  \n",
    "  W_2 = np.random.randn(3, 5)\n",
    "  b_2 = np.random.randn(1, 3)\n",
    "\n",
    "  # Defining the learning rate\n",
    "\n",
    "  lr = 1e-3\n",
    "\n",
    "  for iteration in range(num_iteration):\n",
    "      # Forward Propagation\n",
    "\n",
    "    Z_1 = np.dot(X, W_1.T) + b_1\n",
    "    A_1 = sigmoid(Z_1)\n",
    "    Z_2 = np.dot(A_1, W_2.T) + b_2\n",
    "    Y_hat = softmax(Z_2)\n",
    "\n",
    "    # Backward Propagation\n",
    "\n",
    "    dE_dY_hat = Y_hat - Y\n",
    "    dY_hat_dW_2 = A_1\n",
    "\n",
    "    dE_dW_2 = np.dot(dY_hat_dW_2.T, dE_dY_hat)\n",
    "\n",
    "    dE_db_2 = dE_dY_hat\n",
    "\n",
    "    dZ_2_dA_1 = W_2\n",
    "    dE_dA_1 = np.dot(dE_dY_hat, dZ_2_dA_1)\n",
    "    dA_1_dZ_1 = sigmoid_der(Z_1)\n",
    "    dZ_1_dW_1 = X\n",
    "    dE_dW_1 = np.dot(dZ_1_dW_1.T, dA_1_dZ_1 * dE_dA_1)\n",
    "\n",
    "    dE_db_1 = dE_dA_1 * dA_1_dZ_1\n",
    "\n",
    "    # Gradient Descent\n",
    "\n",
    "    W_2 = W_2 - lr * dE_dW_2.T\n",
    "    b_2 = b_2 - lr * dE_db_2.sum(axis=0)\n",
    "\n",
    "    W_1 = W_1 - lr * dE_dW_1.T\n",
    "    b_1 = b_1 - lr * dE_db_1.sum(axis=0)\n",
    "\n",
    "\n",
    "    if iteration % 5 == 0:\n",
    "      print(\"iteration : \", iteration, \"   loss : \", loss(Y, Y_hat))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  X, Y = datasets.load_iris(return_X_y=True)\n",
    "\n",
    "  # Preprocessing the data to have a mean 0 and variance 1\n",
    "  X = preprocessing.scale(X)\n",
    "  # Y has the shape (150, ) rather than (150, 1)\n",
    "  Y = Y.reshape(150,1)\n",
    "  neural_network_train(X, Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hyperparameters and Parameters](https://towardsdatascience.com/model-parameters-and-hyperparameters-in-machine-learning-what-is-the-difference-702d30970f6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
